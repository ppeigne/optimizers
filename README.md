# optimizers
Gradient descent optimization algoritms

### Sources 

[CS321 - NN3](https://cs231n.github.io/neural-networks-3/)
* Momentum
* Nesterov Momentum
* Adagrad
* RMSprop
* Adam

[Why Momentum Really Works - Goh](https://distill.pub/2017/momentum/)

[Nesterov Accelerated Gradient and Momentum - jlmelville](https://jlmelville.github.io/mize/nesterov.html)

 
[An overview of gradient descent optimization algorithms - Sebastian Ruder](https://ruder.io/optimizing-gradient-descent/index.html)
* Momentum
* Nesterov
* Adagrad
* Adadelta
* RMSprop
* Adam
* AdaMax
* Nadam
* AMSGrad

[Optimization for Deep Learning - Sebastian Ruder](https://www.slideshare.net/SebastianRuder/optimization-for-deep-learning)


[An updated overview of recent gradient descent algorithms - John Chen](https://johnchenresearch.github.io/demon/)
* Momentum
* Adagrad
* RMSprop
* Adam
* AMSGrad
* AdamW
* QHAdam
* YellowFin
* QHM
* Demon

#### To go further
Second order methods
  * L-BFSG
  * SFO (L-BFGS + SGD)
